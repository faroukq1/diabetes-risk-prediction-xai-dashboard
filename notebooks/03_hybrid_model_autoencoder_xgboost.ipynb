{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9996dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Machine Learning\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00495955",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d9a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets from previous notebook\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test = joblib.load(\n",
    "    '../data/processed_datasets.pkl'\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train_scaled.shape}\")\n",
    "print(f\"Validation set: {X_val_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")\n",
    "print(f\"\\nNumber of features: {X_train_scaled.shape[1]}\")\n",
    "\n",
    "# Convert to numpy arrays for TensorFlow\n",
    "X_train_np = X_train_scaled.values\n",
    "X_val_np = X_val_scaled.values\n",
    "X_test_np = X_test_scaled.values\n",
    "\n",
    "print(f\"\\nData converted to numpy arrays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771842ce",
   "metadata": {},
   "source": [
    "## 2. Build Autoencoder Architecture\n",
    "\n",
    "The autoencoder will compress the input features into a lower-dimensional latent space,\n",
    "learning meaningful representations that capture the essential patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b082f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder architecture\n",
    "input_dim = X_train_np.shape[1]\n",
    "encoding_dim = 10  # Latent dimension\n",
    "\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Encoding dimension: {encoding_dim}\")\n",
    "\n",
    "# Encoder\n",
    "input_layer = layers.Input(shape=(input_dim,), name='input')\n",
    "encoded = layers.Dense(32, activation='relu', name='encoder_layer1')(input_layer)\n",
    "encoded = layers.BatchNormalization()(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(16, activation='relu', name='encoder_layer2')(encoded)\n",
    "encoded = layers.BatchNormalization()(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "latent = layers.Dense(encoding_dim, activation='relu', name='latent_space')(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = layers.Dense(16, activation='relu', name='decoder_layer1')(latent)\n",
    "decoded = layers.BatchNormalization()(decoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(32, activation='relu', name='decoder_layer2')(decoded)\n",
    "decoded = layers.BatchNormalization()(decoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "output_layer = layers.Dense(input_dim, activation='linear', name='output')(decoded)\n",
    "\n",
    "# Full autoencoder\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer, name='autoencoder')\n",
    "\n",
    "# Encoder model (for extracting latent features)\n",
    "encoder = Model(inputs=input_layer, outputs=latent, name='encoder')\n",
    "\n",
    "print(\"\\nAutoencoder architecture created\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a9913",
   "metadata": {},
   "source": [
    "## 3. Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c759bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile autoencoder\n",
    "autoencoder.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training autoencoder...\\n\")\n",
    "\n",
    "# Train autoencoder\n",
    "history = autoencoder.fit(\n",
    "    X_train_np, X_train_np,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_np, X_val_np),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Autoencoder training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16591a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Autoencoder Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# MAE plot\n",
    "axes[1].plot(history.history['mae'], label='Train MAE', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Autoencoder Training MAE', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/autoencoder_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal train loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b495e",
   "metadata": {},
   "source": [
    "## 4. Extract Latent Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc472aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latent representations\n",
    "X_train_encoded = encoder.predict(X_train_np, verbose=0)\n",
    "X_val_encoded = encoder.predict(X_val_np, verbose=0)\n",
    "X_test_encoded = encoder.predict(X_test_np, verbose=0)\n",
    "\n",
    "print(f\"Original feature space: {X_train_np.shape[1]} dimensions\")\n",
    "print(f\"Encoded feature space: {X_train_encoded.shape[1]} dimensions\")\n",
    "print(f\"Dimensionality reduction: {(1 - X_train_encoded.shape[1]/X_train_np.shape[1])*100:.1f}%\")\n",
    "\n",
    "# Create DataFrame for encoded features\n",
    "encoded_columns = [f'latent_{i+1}' for i in range(encoding_dim)]\n",
    "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_columns)\n",
    "X_val_encoded_df = pd.DataFrame(X_val_encoded, columns=encoded_columns)\n",
    "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_columns)\n",
    "\n",
    "print(\"\\nLatent features extracted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ff1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latent space (first 2 dimensions)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(X_train_encoded[:, 0], X_train_encoded[:, 1], \n",
    "                     c=y_train, cmap='RdYlGn_r', alpha=0.6, s=50)\n",
    "plt.colorbar(scatter, label='Diabetes (0=Healthy, 1=Diabetic)')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('Latent Space Visualization (First 2 Dimensions)', fontsize=12, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_train_encoded[y_train==0, 0], X_train_encoded[y_train==0, 1],\n",
    "           alpha=0.6, label='Healthy', s=50, color='green')\n",
    "plt.scatter(X_train_encoded[y_train==1, 0], X_train_encoded[y_train==1, 1],\n",
    "           alpha=0.6, label='Diabetic', s=50, color='red')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('Latent Space by Class', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/latent_space_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403961c9",
   "metadata": {},
   "source": [
    "## 5. Train XGBoost on Latent Features (Hybrid Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost on encoded features\n",
    "print(\"Training Hybrid Model (Autoencoder + XGBoost)...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate scale_pos_weight for class imbalance\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "hybrid_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "hybrid_model.fit(\n",
    "    X_train_encoded, y_train,\n",
    "    eval_set=[(X_val_encoded, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"✓ Hybrid model training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67634ef",
   "metadata": {},
   "source": [
    "## 6. Evaluate Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a88abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred_hybrid = hybrid_model.predict(X_train_encoded)\n",
    "y_val_pred_hybrid = hybrid_model.predict(X_val_encoded)\n",
    "y_test_pred_hybrid = hybrid_model.predict(X_test_encoded)\n",
    "\n",
    "# Prediction probabilities\n",
    "y_train_proba_hybrid = hybrid_model.predict_proba(X_train_encoded)[:, 1]\n",
    "y_val_proba_hybrid = hybrid_model.predict_proba(X_val_encoded)[:, 1]\n",
    "y_test_proba_hybrid = hybrid_model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred, y_proba, model_name, dataset_name):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} - {dataset_name} Set\")\n",
    "    print('='*60)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Healthy', 'Diabetic']))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "\n",
    "# Evaluate on all sets\n",
    "hybrid_train_metrics = evaluate_model(y_train, y_train_pred_hybrid, y_train_proba_hybrid,\n",
    "                                     \"Hybrid Model\", \"Train\")\n",
    "hybrid_val_metrics = evaluate_model(y_val, y_val_pred_hybrid, y_val_proba_hybrid,\n",
    "                                   \"Hybrid Model\", \"Validation\")\n",
    "hybrid_test_metrics = evaluate_model(y_test, y_test_pred_hybrid, y_test_proba_hybrid,\n",
    "                                    \"Hybrid Model\", \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399cbf95",
   "metadata": {},
   "source": [
    "## 7. Compare with Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline comparison\n",
    "baseline_comparison = pd.read_csv('../reports/baseline_models_comparison.csv')\n",
    "\n",
    "# Add hybrid model results\n",
    "hybrid_results = pd.DataFrame([\n",
    "    {'Model': 'Hybrid (AE+XGB)', 'Set': 'Train', **hybrid_train_metrics},\n",
    "    {'Model': 'Hybrid (AE+XGB)', 'Set': 'Validation', **hybrid_val_metrics},\n",
    "    {'Model': 'Hybrid (AE+XGB)', 'Set': 'Test', **hybrid_test_metrics}\n",
    "])\n",
    "\n",
    "# Combine all results\n",
    "full_comparison = pd.concat([baseline_comparison, hybrid_results], ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"COMPLETE MODEL COMPARISON (INCLUDING HYBRID)\")\n",
    "print(\"=\"*90)\n",
    "print(full_comparison.to_string(index=False))\n",
    "\n",
    "# Save updated comparison\n",
    "full_comparison.to_csv('../reports/all_models_comparison.csv', index=False)\n",
    "print(\"\\n✓ Comparison saved to ../reports/all_models_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Filter test set results\n",
    "test_results = full_comparison[full_comparison['Set'] == 'Test']\n",
    "\n",
    "# Plot 1: Bar chart of all metrics\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "rf_scores = test_results[test_results['Model'] == 'Random Forest'][metrics].values[0]\n",
    "xgb_scores = test_results[test_results['Model'] == 'XGBoost'][metrics].values[0]\n",
    "hybrid_scores = test_results[test_results['Model'] == 'Hybrid (AE+XGB)'][metrics].values[0]\n",
    "\n",
    "axes[0].bar(x - width, rf_scores, width, label='Random Forest', alpha=0.8)\n",
    "axes[0].bar(x, xgb_scores, width, label='XGBoost', alpha=0.8)\n",
    "axes[0].bar(x + width, hybrid_scores, width, label='Hybrid (AE+XGB)', alpha=0.8)\n",
    "axes[0].set_xlabel('Metrics', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('Test Set Performance: All Models', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels([m.upper() for m in metrics], rotation=45)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "\n",
    "# Plot 2: ROC Curves comparison\n",
    "# Load baseline models\n",
    "xgb_baseline = joblib.load('../data/xgboost_model.pkl')\n",
    "rf_baseline = joblib.load('../data/random_forest_model.pkl')\n",
    "\n",
    "# Get predictions\n",
    "y_test_proba_rf = rf_baseline.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_proba_xgb = xgb_baseline.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate ROC curves\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_test_proba_rf)\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_test_proba_xgb)\n",
    "fpr_hybrid, tpr_hybrid, _ = roc_curve(y_test, y_test_proba_hybrid)\n",
    "\n",
    "axes[1].plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC={rf_scores[4]:.3f})', linewidth=2)\n",
    "axes[1].plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC={xgb_scores[4]:.3f})', linewidth=2)\n",
    "axes[1].plot(fpr_hybrid, tpr_hybrid, label=f'Hybrid (AUC={hybrid_scores[4]:.3f})', linewidth=2.5)\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Guess', linewidth=1)\n",
    "axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[1].set_title('ROC Curves Comparison - Test Set', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/hybrid_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e87e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance in hybrid model\n",
    "feature_importance_hybrid = pd.DataFrame({\n",
    "    'feature': encoded_columns,\n",
    "    'importance': hybrid_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_hybrid['feature'], feature_importance_hybrid['importance'])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Latent Feature', fontsize=12)\n",
    "plt.title('Feature Importance in Hybrid Model (Latent Features)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/hybrid_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLatent Feature Importance:\")\n",
    "print(feature_importance_hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469cc433",
   "metadata": {},
   "source": [
    "## 8. Analysis: Dimensionality Reduction Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4699fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction error analysis\n",
    "X_train_reconstructed = autoencoder.predict(X_train_np, verbose=0)\n",
    "X_test_reconstructed = autoencoder.predict(X_test_np, verbose=0)\n",
    "\n",
    "train_mse = np.mean((X_train_np - X_train_reconstructed) ** 2, axis=1)\n",
    "test_mse = np.mean((X_test_np - X_test_reconstructed) ** 2, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Reconstruction error by class\n",
    "axes[0].hist(train_mse[y_train == 0], bins=30, alpha=0.6, label='Healthy', color='green')\n",
    "axes[0].hist(train_mse[y_train == 1], bins=30, alpha=0.6, label='Diabetic', color='red')\n",
    "axes[0].set_xlabel('Reconstruction Error (MSE)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Reconstruction Error Distribution by Class', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Dimensionality comparison\n",
    "dimensions = ['Original\\n('+str(input_dim)+' features)', \n",
    "             'Encoded\\n('+str(encoding_dim)+' features)']\n",
    "dim_values = [input_dim, encoding_dim]\n",
    "colors = ['skyblue', 'orange']\n",
    "\n",
    "bars = axes[1].bar(dimensions, dim_values, color=colors, alpha=0.7)\n",
    "axes[1].set_ylabel('Number of Features', fontsize=12)\n",
    "axes[1].set_title('Feature Space Dimensionality', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/autoencoder_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nReconstruction Error Statistics (Train):\")\n",
    "print(f\"Healthy - Mean: {train_mse[y_train==0].mean():.4f}, Std: {train_mse[y_train==0].std():.4f}\")\n",
    "print(f\"Diabetic - Mean: {train_mse[y_train==1].mean():.4f}, Std: {train_mse[y_train==1].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef7a539",
   "metadata": {},
   "source": [
    "## 9. Save Hybrid Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7fc1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "autoencoder.save('../data/autoencoder_model.h5')\n",
    "encoder.save('../data/encoder_model.h5')\n",
    "joblib.dump(hybrid_model, '../data/hybrid_xgboost_model.pkl')\n",
    "\n",
    "# Save encoded datasets\n",
    "joblib.dump((X_train_encoded, X_val_encoded, X_test_encoded), \n",
    "           '../data/encoded_datasets.pkl')\n",
    "\n",
    "print(\"✓ Hybrid model components saved:\")\n",
    "print(\"  - Autoencoder: ../data/autoencoder_model.h5\")\n",
    "print(\"  - Encoder: ../data/encoder_model.h5\")\n",
    "print(\"  - XGBoost: ../data/hybrid_xgboost_model.pkl\")\n",
    "print(\"  - Encoded data: ../data/encoded_datasets.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd3295f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Hybrid Model Performance:\n",
    "The autoencoder successfully reduced dimensionality from the original feature space to a compact latent representation,\n",
    "while maintaining predictive performance.\n",
    "\n",
    "### Key Benefits:\n",
    "1. **Dimensionality Reduction**: Compressed features for more efficient modeling\n",
    "2. **Feature Learning**: Learned meaningful representations automatically\n",
    "3. **Noise Reduction**: Autoencoder filtering can reduce noise in the data\n",
    "4. **Interpretability**: Latent features capture essential patterns\n",
    "\n",
    "### Comparison with Baselines:\n",
    "- The hybrid model performance should be compared with baseline XGBoost on original features\n",
    "- Trade-off between model complexity and performance\n",
    "- Computational efficiency gained from reduced dimensionality\n",
    "\n",
    "✓ Part 2.2 (Hybrid Model) completed!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
